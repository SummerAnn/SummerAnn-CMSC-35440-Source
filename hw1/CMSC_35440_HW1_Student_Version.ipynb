{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cf4a7c",
   "metadata": {},
   "source": [
    "# CMSC 35440 Machine Learning in Biology and Medicine\n",
    "## Homework 1: Embedding Immunology Research Articles\n",
    "**Released**: Jan 12, 2026\n",
    "\n",
    "**Due**: Jan 23, 2026 at 11:59 PM Chicago Time on Gradescope\n",
    "\n",
    "**In this first homework, you'll generate embeddings for 28 immunology research articles and visualize them using various dimensionality reduction techniques.**\n",
    "\n",
    "At a high-level, embeddings are vectors computed by some algorithm or model that \"code\" information from data. For this homework, you will code text documents as vectors using the bag of words algorithm and normalize these vectors using the term-frequency inverse document frequency (TF-IDF) method. TF-IDF downweights ubiquitous terms and highlights vocabulary that is distinctive to each paper (e.g., checkpoint, cGAS-STING, germinal center). This helps the embeddings reflect biological themes rather than common filler, so downstream plots can separate immune subfields and detect cross-cutting topics.\n",
    "\n",
    "The 28 papers span three major areas of immunology:\n",
    "- **T-cell biology**: CD8+ T cell exhaustion, checkpoint inhibition, cancer immunotherapy\n",
    "- **B-cell biology**: Germinal centers, antibody responses, T follicular helper cells  \n",
    "- **Innate immunity**: TLRs, cGAS-STING pathway, macrophages, autophagy\n",
    "\n",
    "All papers are from journals like *Immunity*, *Cell*, *Nature Reviews*, and *Annual Reviews* (2015-2024).\n",
    "\n",
    "For this homework, you will code text documents as vectors using the bag of words algorithm and normalize these vectors using the term-frequency inverse documentation frequency (TF-IDF) method. This method dates back over 50 years to 1972. Through this homework, hopefully we'll convince you that it's still very much relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae803e",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. Download and open this starter notebook in your favorite Jupyter Notebook host. We recommend using [Google Colab](https://colab.research.google.com/).\n",
    "   * **NB:** We'll design all homeworks such that they can be run on the *free* tier of Colab.\n",
    "   * For this homework, we don't require the use of any GPUs.\n",
    "\n",
    "2. Download and unzip the research articles. We've provided them as a tarball that can be downloaded from [https://github.com/SummerAnn/SummerAnn-CMSC-35440-Source/releases/download/hw1/hw1.tar.gz](https://github.com/SummerAnn/SummerAnn-CMSC-35440-Source/releases/download/hw1/hw1.tar.gz).\n",
    "   * You'll notice that there's a CSV of article metadata and a folder of article *PDFs*. While these articles are available elsewhere on the internet as extracted-text, real-world data is messy. One such way that data can be messy is that it only exists as PDFs - so **you must use the article PDFs for this assignment**.\n",
    "\n",
    "3. Extract the text from the articles. You should probably use some variables from the metadata at this step.\n",
    "\n",
    "4. Compute the term-document matrix and normalize using TF-IDF. **You must implement TF-IDF yourself. You may not use any existing implementations** (e.g. you can NOT use sklearn's TfidfVectorizer).\n",
    "   * Defining what is a \"term\" is up to you but don't overcomplicate it. Splitting on whitespace characters works fine.\n",
    "   * The Wikipedia article should be all you need: [https://en.wikipedia.org/wiki/Tf-idf](https://en.wikipedia.org/wiki/Tf-idf).\n",
    "\n",
    "5. Normalize your per-document embeddings using L2 normalization.\n",
    "\n",
    "6. Visualize your embeddings using dimensionality reduction (3 plots total):\n",
    "   * Apply linear dimensionality reduction\n",
    "   * Apply a **non-linear** method \n",
    "   * Add a **clustermap**  for a global similarity view\n",
    "   * **Important for all plots**: Color points by the **subtopic** column in the metadata CSV to show how well your embeddings capture biological themes\n",
    "   * **Important**: Many methods are sensitive to hyperparameters. See: [https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/)\n",
    "   * Label your plots clearly with titles, axis labels, and a legend showing which color corresponds to which subtopic\n",
    "\n",
    "7. Analyze your results and submit:\n",
    "   * Your submission should include 2 things:\n",
    "     1. Your writeup containing figures with your embedding visualizations\n",
    "     2. Your notebook with your code for computing TF-IDF and generating figures\n",
    "   * Your careful writeup \n",
    "   * Some guiding questions: Have your embeddings captured underlying information about the articles? Why do some articles cluster together? How do the three figures you generated compare? Do papers with the same subtopic cluster together?\n",
    "\n",
    "**Tips and Tricks:**\n",
    "1. You're welcome to use any tools except where noted above.\n",
    "2. Reading CSVs: use `pandas`\n",
    "3. Extracting text from PDFs: use [`pypdf`](https://github.com/py-pdf/pypdf)\n",
    "4. Normalization: use `numpy`\n",
    "5. Visualization: use `matplotlib` or `seaborn`\n",
    "6. For help: Email course staff or come to office hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacd174",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf numpy pandas matplotlib seaborn scikit-learn umap-learn\n",
    "!wget https://github.com/SummerAnn/SummerAnn-CMSC-35440-Source/releases/download/hw1/hw1.tar.gz\n",
    "!tar -xzf hw1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import tab10\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import umap\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "globus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
