{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CMSC 35440 Machine Learning in Biology and Medicine\n## Homework 1: Embedding Immunology Research Articles\n**Released**: Jan 14, 2026\n\n**Due**: Jan 24, 2026 at 11:59 PM Chicago Time on Gradescope\n\n**In this first homework, you'll generate embeddings for 28 immunology research articles and visualize them using various dimensionality reduction techniques.**\n\nAt a high-level, embeddings are vectors computed by some algorithm or model that \"code\" information from data. For this homework, you will code text documents as vectors using the bag of words algorithm and normalize these vectors using the term-frequency inverse document frequency (TF-IDF) method. TF-IDF downweights ubiquitous terms and highlights vocabulary that is distinctive to each paper (e.g., checkpoint, cGAS-STING, germinal center). This helps the embeddings reflect biological themes rather than common filler, so downstream plots can separate immune subfields and detect cross-cutting topics.\n\nThe 28 papers span three major areas of immunology:\n- **T-cell biology**: CD8+ T cell exhaustion, checkpoint inhibition, cancer immunotherapy\n- **B-cell biology**: Germinal centers, antibody responses, T follicular helper cells  \n- **Innate immunity**: TLRs, cGAS-STING pathway, macrophages, autophagy\n\nFor this homework, you will code text documents as vectors using the bag of words algorithm and normalize these vectors using the term-frequency inverse documentation frequency (TF-IDF) method. This method dates back over 50 years to 1972. Through this homework, hopefully we'll convince you that it's still very much relevant."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Instructions\n\n1. Download and open this starter notebook in your favorite Jupyter Notebook host. We recommend using [Google Colab](https://colab.research.google.com/).\n   * **NB:** We'll design all homeworks such that they can be run on the *free* tier of Colab.\n   * For this homework, we don't require the use of any GPUs.\n\n2. Download and unzip the research articles. We've provided them as a tarball that can be downloaded from [https://github.com/SummerAnn/SummerAnn-CMSC-35440-Source/releases/download/hw1/hw1.tar.gz](https://github.com/SummerAnn/SummerAnn-CMSC-35440-Source/releases/download/hw1/hw1.tar.gz).\n   * You'll notice that there's a CSV of article metadata and a folder of article *PDFs*. While these articles are available elsewhere on the internet as extracted-text, real-world data is messy. One such way that data can be messy is that it only exists as PDFs - so **you must use the article PDFs for this assignment**.\n\n3. Extract the text from the articles. You should probably use some variables from the metadata at this step.\n\n4. Compute the term-document matrix and normalize using TF-IDF. **You must implement TF-IDF yourself. You may not use any existing implementations** (e.g. you can NOT use sklearn's TfidfVectorizer).\n   * Defining what is a \"term\" is up to you but don't overcomplicate it. Splitting on whitespace characters works fine.\n   * The Wikipedia article should be all you need: [https://en.wikipedia.org/wiki/Tf-idf](https://en.wikipedia.org/wiki/Tf-idf).\n\n5. Normalize your per-document embeddings using L2 normalization.\n\n6. Visualize your embeddings using dimensionality reduction (3 plots total):\n   * Apply linear dimensionality reduction\n   * Apply a **non-linear** method \n   * Add a **clustermap** for a global similarity view\n   * **Important for all plots**: Color points by the **subtopic** column in the metadata CSV to show how well your embeddings capture biological themes\n   * **Important**: Non-linear methods are sensitive to hyperparameters. See: [https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/)\n   * Label your plots clearly with titles, axis labels, and a legend showing which color corresponds to which subtopic\n\n7. Analyze your results and submit:\n   * Your submission should include 2 things:\n     1. Your writeup containing figures with your embedding visualizations\n     2. Your notebook with your code for computing TF-IDF and generating figures\n   * Your writeup should be **0.5 to 1 page** (before figures). Text should be 12pt, single spaced, with 1 inch margins, on letter size paper. PDF or Word.\n   * Some guiding questions: Have your embeddings captured underlying information about the articles? Why do some articles cluster together? How do the three figures you generated compare? Do papers with the same subtopic cluster together?\n\n**Tips and Tricks:**\n1. You're welcome to use any tools except where noted above.\n2. Reading CSVs: use `pandas`\n3. Extracting text from PDFs: use [`pypdf`](https://github.com/py-pdf/pypdf)\n4. Normalization: use `numpy`\n5. Visualization: use `matplotlib` or `seaborn`\n6. For help: Email course staff or come to office hours!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf numpy pandas matplotlib seaborn scikit-learn umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install pypdf numpy pandas matplotlib seaborn scikit-learn umap-learn\n!wget https://github.com/SummerAnn/SummerAnn-CMSC-35440-Source/releases/download/hw1/hw1.tar.gz\n!tar -xzf hw1.tar.gz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Prefer local metadata (with subtopic if available)\n",
    "candidates = [\n",
    "    \"article-metadata-with-subtopic.csv\",\n",
    "    \"hw1/article-metadata-with-subtopic.csv\",\n",
    "    \"article-metadata.csv\",\n",
    "    \"hw1/article-metadata.csv\",\n",
    "]\n",
    "meta_path = next((p for p in candidates if os.path.exists(p)), candidates[-1])\n",
    "df = pd.read_csv(meta_path)\n",
    "\n",
    "label_col = \"subtopic\" if \"subtopic\" in df.columns else (\"topic\" if \"topic\" in df.columns else (\"category\" if \"category\" in df.columns else df.columns[-1]))\n",
    "label_title = label_col.capitalize()\n",
    "print(f\"Loaded {len(df)} papers from {meta_path}\")\n",
    "print(f\"\\n{label_title} distribution:\")\n",
    "print(df[label_col].value_counts())\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "globus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}